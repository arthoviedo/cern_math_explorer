\chapter{Evaluation}
...

\section{Dataset}
Our dataset consists on 12234 latex documents fetched from ArXiV. This documents correspond to the latest records harvested for CDS. The documents were processed using LateXML as described in the extraction process. From the given dataset, the simpler equations were filtered out (The ones containing one or two elements).


\section{Setup}

The machine used for experiments has a Intel Core i7 processor running at 3.4Ghz with 4 cores. The available RAM is 8Gb, and the Java Virtual Machine used for running Solr was run with the $-Xms2G -Xmx5G$ parameters. The complete specifications of the used software is presented in table \ref{sw_used}

\begin{longtable}{|m{2cm}|>
{\centering\arraybackslash}m{4cm}|>
{\centering\arraybackslash}m{4cm}|
}
\hline 
\textbf{Software} & 
\textbf{Version} 
\\
\hline
Operative System & Linux Mint 16 Petra (Ubuntu based) \\ \hline
Linux Kernel & 3.11.0-15 \\ \hline
Java Virtual Machine & 1.7.0-45 \\ \hline
Apache Solr & 4.6 \\ \hline
Python & 2.7.5+ \\ \hline
\caption{Software used during evaluations}
\label{sw_used}
\end{longtable}

\section{Results quality}
In this section we are interested in evaluating the quality of the results provided by our system.
The type of queries that $5e^{x+y}$ was designed for, are typical equations that can be found in a physics scientific publication. 
Out main user case is that a user of CDS is interested in finding documents inside the collection whose equations are related to the one that is provided.  

\subsection{Equations}
The evaluated queries were provided by physics experts working at CERN from their own research topics. The queries were provided in \LaTeX format as the users used them in their own publications.
Table \ref{equations_table} presents the set of queries that were evaluated, and a simple explanation about it. 
Each of the users evaluated the results only for the equations he/she provided. For each of the presented result, the user ranked it as "Strictly Relevant", "Partially Relevant" and "Completely Irrelevant". As all evaluations including human interaction, this scoring is at some level subjective. 

We present results for three metrics: Partial precision (Document either partially relevant or completely relevant) strict precision and discounted cumulative gain.




\section{Comparison against other systems}
In this section we compared our implementation of MASE(CDS) with the public available systems presented in the related work chapter. We judged if the fetched results of each system were relevant or not using the same criteria as for the previous section.  The systems were tested using Firefox 26.0
Table \ref{comparison_sw_table} presents the results of our test.

\begin{figure}
\includegraphics[height=15 cm]{figures/comparison_table.png}
\label{comparison_sw_table}
\caption{Results of comparison of available mathematical search systems}
\end{figure}

The first result about this evaluation is that only two different instances developed and running. The other ones seems to have been abandoned.

\section{Querying Performance}


\section{Indexing Performance}
For this set of evaluations, we were only interested in observing how the performance of the indexing stage is affected by the different types of features that are employed. We selected the first 100 records from our dataset and measured the total time it required to index all the elements in Solr using a python script and the solrpy library to do the communication. The default similarity in Solr was used. We also measured the index size at the end of each experiment.
Our four scenarios were organized as follows:
\begin{itemize}
\item N : Only notational features over the original expression were used
\item N+S : Notational and structural features over the original expression
\item N+S+NN : Notational and structural features over the original expression and notational features over the normalized string using Mathematica's Simplify function
\item N+S+FN : Notational and structural features over the original expression and notational features over the normalized string using Mathematica's Full Simplify function
\end{itemize}

Table \ref{indexing_performance} summarizes the results of this set of experiments. The first result is the significant increase in processing time when using Mathematica. Going from the N scenario to N+S represent an increase of almost 15x in the indexing time. Adding standard normalization increases the indexing time by a factor near to 1.44x and full normalization by a factor of 1.55x.

With respect to storage size, we see a small footprint (Less than 5\%)in the total size by going from N to N+S. Adding normalization represents an increase by a factor of 1.78x and the size of the fully normalized index represents a smaller increase (1.76x)

We can observe that there is a small trade-off between time and storage size depending on which normalization mode is employed.

\begin{longtable}{|c|c|p{2cm}|p{2cm}|p{2cm}|}
\hline 
Feature Set & Total time (sec) & Index Size (kb) & Avg. time per equation & Avg. size of equation (kb) \\ 
\hline 
N & 58 & 13926 & 0.00524 & 1.237 \\ 
\hline 
N+S & 884 & 14438 & 0.0785 & 1.282 \\ 
\hline 
N+S+NN & 1273 & 25780 & 0.113 & 2.345 \\ 
\hline 
N+S+FN & 1377 & 25548 & 0.122 & 2.269 \\ 
\hline
\caption{Indexing performance}
\label{indexing_performance}
\end{longtable} 



\section{Alternative Metrics}
According to some experimental evaluations presented in \cite{tree_comparison}, Tree Edit Distance provides an interesting alternative metric to the standard vector space model using angular distance. In this work, authors present an improvement in the quality of retrieved results compared to the classic approach of indexing the single leaves of the tree and a subset of the possible sub-trees. For evaluating the feasibility of this approach we compared a set of equations using both metrics: Tree edit distance (Using Zheng and Shasha's algorithm implemented in python \cite{tree_distance_python}.) and a standard angular distance that was not heavily optimized. We compared elapsed times for expressions of different length.


%%
%\begin{longtable}{|m{3.5cm}|>
%{\centering\arraybackslash}m{3cm}|>
%{\centering\arraybackslash}m{3cm}|>
%{\centering\arraybackslash}m{5cm}|
%}
%\hline 
%\textbf{\LaTeX} & 
%\textbf{Presentation MathML} & 
%\textbf{Content MathML + Presentation Markups} & 
%\textbf{OpenMath}  \\ \hline
%%%

\begin{longtable}{|>{\centering\arraybackslash}m{3.5cm}|>
{\centering\arraybackslash}m{3cm}|>
{\centering\arraybackslash}m{3cm}|
}
\hline 
Expression Length (Number of leaf nodes)& Angular Distance Time (msec) & Tree Edit Distance Time(msec) \\ \hline 
3 & 0.01195 & 2.65  \\ \hline
8 & 0.0142 & 9.97  \\ \hline
7 & 0.0129 & 9.165  \\ \hline
12 & 0.0194 & 15.15  \\ \hline
18 & 0.0201 & 29.5  \\ \hline
20 & 0.0245 & 27.75  \\ \hline
22 & 0.0264 & 32.4  \\ \hline
24 & 0.02502 & 32.1 \\ \hline

\caption{Distance computation using two different metrics}
\label{comparison_tree_times}
\end{longtable} 

This result show that for shorter expressions, using tree edit distance represent an increase in time in more than 200x, and a factor of around 1200x for the longer expressions, making it impractical for implementing a system that needs to be responsive. Even though there are algorithms which provide a better asymptotic guarantees and better actual running times like RTED\cite{rted} with respect to Zheng ans Shasha's algorithm, the best improvements represent a decrease in processing time of 50\% to 80\% in trees with around 1000 nodes, while the standard tested equations which are representative of our dataset, contain less than 100 nodes.










