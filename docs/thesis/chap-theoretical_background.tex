\chapter{Theoretical Background}
\label{chapter-theoretical_background}
Information Retrieval (IR) consists of the activity of identifying relevant documents from a collection based on some input parameters.
Different models have been developed through the evolution of the field. In the subsequent sections, the most relevant models to this work are presented.

\section{Boolean Model}
The Boolean model of Information Retrieval (BIR) was the first one to be developed and is currently one of the most used in the current implementation of IR systems because of its simplicity. It is based on the mathematical concepts of Boolean Logic and Set Theory. A formalization of this framework can be stated as follows: 

Given a set of terms $T = {t_1, t_2, ... , t_n}$ which are the terms that will be indexed (In a standard search engine this can be the set of words in a language after some normalization steps), a set of documents $D = {d_1, d_2, ... , d_n} $ where $d_i \in \mathcal{P}(T) $ that the user will search for. 
A query element $q$ is a Boolean expression over the index terms and the operators $AND$ , $OR$ and $NOT$. We define the predicate $ Relevant(q, d_i) $ if a document $d_i$ satisfies the query expression $q$.
Finally the result for query over a collection of documents can now be written as $Result(q, D) = \{d | Relevant(q, d)\}$
The benefits of this model are its simplicity, both in its formalism and in its implementation. 
The main disadvantage of  BIR are the fact that all terms are given the same importance which makes it difficult to rank results. Since all results have same importance, it is difficult also to limit the number of retrieved documents and therefore the output set can be difficult to process for the final user.

One of the key data structures for an efficient implementation of this model is the inverted index where for a given term, the index stores which documents contain it. This structure is used to efficiently solve queries by only scanning document that contain at least one of the terms in the query. Scanning a document can be done by using a standard (Also called forward) index, where for a given document, the index stores the terms associated with it. Both indexes can be implemented using hash tables.

\section{Vector Space Model}
The vector space model\cite{vector_space_model} which was developed between the 1960s and 1970s alongside the SMART Information Retrieval System. In this model, documents (from a collection $D$) and queries are represented as vectors $d$ and $q$ in a vector space where each index term corresponds to a dimension. If a term $t_i$ occurs in a given query or document, its i-th  coordinate will have a non zero value. Index terms can be keywords or phrases in the context of textual search or any other type of feature that can be extracted from the elements in a given specific context.
The scoring of a document for a given query is computed as the cosine of the angle between both vectors which can be expressed as: 
$$score(d, q) = cos\theta(d, q) = \frac{d \cdot q}{\parallel d \parallel \parallel q \parallel}  = \frac{\sum_{i=1}^{n}weight(d_i, d)*weight(q_i, q)}{\sqrt{\sum_{i=1}^{n}weight^2(d_i, d)}\sqrt{\sum_{i=1}^{n}weight^2(q_i, q)}} $$

Different weighting schemes can be employed, and the most standard one is called tf-idf (Term frequency - Inverse document frequency) where terms that appear less frequently in the corpus, are assigned higher values (Intuitively the term "the" will be less useful for identifying a relevant document than the term "Shakespeare" in the context of an English word based search engine). Therefore:

$$weight(t, d) = tf(t,d) idf(t)$$

There are several variations for each of the two components. Some approaches for $tf$ include the raw frequency; Boolean frequency: $tf(t,d) = 1$ if $t$ occurs in $d$ or 0 if not; logarithmically scaled: $tf(t,d) = 	\log(frequency(t,d) +1)$
and the augmented frequency: $tf(t,d) = 0.5 + \frac{0.5*frequency(t,d)}{ \max\{frequency(t^\prime,d) | t^\prime \in d\} }$. For computing the inverse document frequency the standard approach follows $idf = \log \frac{|D|}{1+\{d \in D: t \in d \}} $

The main drawbacks of this approach reside in the assumption that the terms are independent.

\section{Probabilistic Relevance Model}
The probabilistic relevance model was developed in 1976 by Robertson and Jones\cite{probabilistic_relevance_model}. The similarity of a document $d$ to a query $q$ is given by the probability of $d$ being relevant to $q$. It assumes there is a set $R$ that is preferred to be the answer set for $q$. The model assumes that the document in $R$ are relevant to $q$ and the documents in $\bar{R}$ are not relevant. $R$ then is the set that maximizes:

$$sim(d,q) = \frac{ P(R|d)} {P(\bar{R} |d)} $$

The importance of this model is that it serves as a solid base for state of the art information retrieval models.

\section{Okapi BM25}

The Okapi BM25 ranking function is one of the current state-of-the-art models in information retrieval\cite{okapibm25}. The actual scoring function is called BM25 and Okapi refers to the first system implementing this function in the 1980s. 
Given a query $q$ as a vector of keywords $q_{1}, q_{2}, ... , q_{n}$ then the score for a document $d$ is:

$$score(d, q) = \sum_{i=1}^{n}IDF{(q_i)}*\frac{ frec(q_i, D)}{frec(q_i, D)+k_1*((1-b)+b*\frac{|D|}{avgdl})} $$
and $$IDF(q_i) = \log{\frac{N - df(q_i)+0.5}{df(q_i)+0.5}} $$

where $frec(q_i, D)$ is the number of occurrences of keyword $q_i$ in $D$, $avgdl$ is the average length (In keywords) of a document, and $k_1$ and $b$ are free parameters. Usually $k_1 = 2$ and $b \in [0, 1]$ controls how important is the length normalization, being set normally to $0.75$. $N$ is the size of the collection, $df(q_i)$ is the number of documents that contain term $q_i$ 

A further variation that takes into account structure in the form of a document containing different fields and each of them having its own length and importance (boost) can be formulated as follows, known as BM25F can be expressed as follows:


$$score(d, q) = \sum_{i=1}^{n} \frac{weigth(q_i, d)}{k+ weigth(q_i, d)} * IDF(q_i)$$
and 
$$weight(q_i, d) = \sum_{j=1}^m \frac{frec(q_i, D, field_j)*boost_j)}{((1-b_j) + b_j*\frac{|l_j|}{avgfl_j})} $$
where $m$ is the number of different fields, $boost_j$ is the relative importance for each field, $b_j$ is analogous to the $b$ constant in the BM25 group of equations, $|l_j|$ is the length of the $field_j$ and $avgfl_j$ is the average length of the $field_j$.

\section{Latent Semantic Indexing}

\section{Latent Dirichlet Allocation}

\section{Tree Edit Distance}
As it will be discussed, one of the most common formats to represent mathematical equations, MathML, is a XML based format, and whose structure represents a rooted tree. Because of this, it makes sense to consider some models to compare and analyse mathematical expressions taking into account their natural tree structure. The most natural way of addressing this, is through the well studied abstract problem of Tree Edit Distance problem, where the idea is to compute a distance between a pair of trees. It should be noted that the development of this problem is very similar to the String Edit Distance problem.
The Tree Edit Distance problem can be formulated as follows:
Let $\Sigma$ be a finite alphabet and let $\lambda \notin \Sigma$ be a special empty symbol. Finally let $\Sigma_\lambda = \Sigma \cup \lambda$. 
Given two trees $T_1$ and $T_2$ that are labelled (That is, each node $N$ in the tree has assigned a label $l(N) \in \Sigma$ ) and ordered (That is, we are provided an order between sibling nodes), we define the following operations ($l(N_1) \rightarrow l(N_2)$) where $(l(N_1),l(N_2)) \in ((\Sigma \cup \lambda)\times(\Sigma \cup \lambda)\setminus(\lambda,\lambda)) $: \\
\textbf{Relabelling:} If $l(N_1) \neq \lambda \wedge l(N_2) \neq \lambda $. We change the label of a node in the tree $T$ \\
\textbf{Deletion:} If $l(N_2) = \lambda $. The children of $N_1$ become the children of $N_1$'s parent ($N_1$ is not the root of the tree) and occupy its position in the sibling ordering. \\
\textbf{Insertion:} If $l(N_1) = \lambda $. This is the complement operation of deletion. \\

For each of this operations $\omega = (l(N_1) \rightarrow l(N_2))$, we assign a cost function $cost(\omega)$ and for a given sequence of this transformations $\Omega = (\omega_1, \omega_2, ... , \omega_n)$ we assign a cost function $cost(\Omega) = \sum_{i=1}^{i=n} {cost(\omega_i)}$ 
We also assume that $cost(\omega)$ is a distance metric. 
With the previous definition, the distance between $T_1$ and $T_2$ can be expressed now as $ dist(T_1, T_2) = min\left\{ cost(\Omega)| \Omega \text{ is a sequence of operations that transform } T_1 \text{ into } T_2 \right\} $

A recursive formulation of the problem can be defined in the following way:

\begin{align*}
dist(T,T) &= 0 \\
dist(F_1, T) &= dist(F_1 - v, T) + cost(v \rightarrow \lambda) \\
dist(T, F_2) &= dist(T, F_2 - w) + cost(\lambda \rightarrow w) \\
dist(F_1, F_2)&=\left\{
\begin{array}{ll}
dist(F_1 - v, F_2) + cost(v \rightarrow \lambda) \\
dist(F_1, F_2 - w) + cost(\lambda \rightarrow w) \\
dist(F_1(v), F_2(w)) + dist(F_1 - T_1(v), F_2 - T_2(w)) + cost(v \rightarrow w)
\end{array}
\right.
\end{align*}  
Where $F$ is a forest, $v$ and $w$ nodes from a free, $F - v$ the forest $F$ with the deletion of the node $v$, $T(v)$  the tree rooted at $v$ and $F-T(v)$ the forest obtained by removing all the nodes in $T(v)$ from $F$	

The presented set of equations give a simple way to compute the edit distance between a pair of trees, and the most known algorithms like Zhang and Shasha's\cite{zhang_shasha} whose worst time case time bound is $O(|T_1|^2|T_2|^2)$, Klein's\cite{klein} that achieves a lower bound of $O(|T_1|^2|T_2|log|T_2|)$ or Demaine's\cite{demaine}, take into advantage the fact that the recursion relies in the solution of smaller sub-problems, so by carefully choosing which of this sub-problems to solve.


\thispagestyle{empty}



