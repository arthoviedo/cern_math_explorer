
\chapter{MASE(CDS)}
\label{chapter-cern_math_explorer}
In this chapter we present our solution called MASE(CDS), Math Aware Search Engine (For CDS). We present the key points of our system, and how they are related to previous built systems.

\section{Features Extraction}
Our main goal is to develop a system that compliments the already available search capabilities in Invenio and in particular the CDS instance. Our system was developed on top of the  the Apache Solr/Lucene framework. CDS already uses Solr as a search engine layer, so developing MASE using this technology supposed a reasonable choice. One of the main steps while building an IR tool, is understanding the data and what kind of features can be extracted from it: If the system is based on text, then tasks as stemming, handling of synonyms, misspelling and so on, such be taken into consideration; if the system being developed is based on images, then examples of features can be histograms of colors, geometrical features and so on. During the literature review, we discovered that most of the projects handled the notational aspects of the expressions, that is, the real naming of the terms in a given equation and each project developed its own set of heuristics to handle the structural component. 


\subsection{Notational Features}
Notational features correspond to the elements that relate to the actual naming of the variables, constants and numeric quantities found in an equation. Here we extract tokens that represent single leaf nodes (E.g. <mi>, <mn>, <mo> tags), and simple constructs like subscript and superscript elements, fractions, roots.
We also extract bi-grams and tri-grams as analogue to that is done for text indexing in current systems.

After examining a set of relevant documents from our dataset, we identified some heuristics that could be applied to improve the recall of our system:



\subsubsection{Unicode Normalizing} 
The first issue we discovered is that because of the big wide of different characters to represent the possible mathematical symbols, there is need for an automatic way to match different possible variations of a single character. An example of this situation is specification of an natural number, sometimes denoted by the letter N. In this case, at least three different common representations were identified: $ N = 1 $ (Using character with unicode code point 0x004e), $\mathcal{N} = 1$ (0xd835 0xdc41 or in latex expressed as \lstinline|\mathcal{N}|) and $\mathbb{N} = 1$ (0x2115 or in latex expressed as \lstinline|\mathbb{N}|). \\
  For addressing this situation, we rely on the Unicode equivalence framework. In the unicode specification, sometimes the same character have two or more different code-points because of backwards compatibility with other encoding standards or because the same character has different essential meanings. Another group of equal characters with different code-points are pre-combined characters such as the Latin letter \~{N} which has Unicode point 0x00D1 and also is the sequence of code-points  0x004E (Latin letter N) and 0x0303 (the combining character tilde). Taking this sample into account, Unicode defines the character composition and decomposition operations as replacing the decomposed representation with the pre-composed one and vice-versa. Finally, Unicode defines two types of equivalences between characters: Canonical and compatible. Canonical equivalent characters are assumed to have the same appearance and meaning. Compatible ones are allowed to have slightly different graphical representations but the same meaning in some contexts. One example of compatible equivalent characters is the Roman number 12 {\unicodefontⅫ} with code-point 0x216b and the sequence of characters XII with codepoints 0x0058 0x0049 0x0049. Taking into account the composition or decomposition of characters and the canonical or compatible equivalences, Unicode specifies 4 different forms of a given character:
  \begin{itemize}
  \item NFD: Normalization Form Canonical Decomposition
  \item NFC: Normalization Form Canonical Composition
  \item NFKD: Normalization Form Compatibility Decomposition
  \item NFKC: Normalization Form Compatibility Composition
  \end{itemize}
  
Table \ref{unicode_characters} presents a small list of troubling characters classes that were identified in our datasets.

\begin{longtable}{|m{4cm}|>{\centering\arraybackslash}m{2.15cm}|>{\centering\arraybackslash}m{1.65cm}|>{\centering\arraybackslash}m{1.65cm}|>{\centering\arraybackslash}m{1.65cm}|>{\centering\arraybackslash}m{1.65cm}|}
\hline 
\textbf{Character Name} & \textbf{Visual Rendering} & \textbf{NFC} & \textbf{NFD} & \textbf{NFKC} & \textbf{NFKD} \\ 
\hline 
LATIN CAPITAL LETTER A & A & 0x41 & 0x41 & 0x41 & 0x41 \\ 
\hline 
LATIN CAPITAL LETTER A WITH MACRON & \={A} & 0x100 & 0x41 0x304 & 0x100 & 0x41 0x304 \\
\hline 
LATIN CAPITAL LETTER A WITH RING ABOVE & \r{A} & 0xc5 & 0x41 0x30a & 0xc5 & 0x41 0x30a \\
\hline 
ANGSTROM SIGN (0x212b) &	\r{A} &	0xc5 &	0x41 0x30a &	0xc5 &	0x41 0x30a \\
\hline
LATIN CAPITAL LIGATURE IJ &	Ĳ &	0x132 &	0x132 &	0x49(I) 0x4a(J) &	0x49 0x4a \\
\hline
CYRILLIC CAPITAL LIGATURE A IE & {\unicodefont Ӕ} &	0x4d4 &	0x4d4 &	0x4d4 &	0x4d4 \\
\hline
LATIN CAPITAL LETTER AE	& {\unicodefont Ӕ}	& 0xc6	& 0xc6	& 0xc6	& 0xc6 \\
\hline
DOUBLE-STRUCK CAPITAL N	& {\unicodefont ℕ}	& 0x2115 & 0x2115 & 0x2115 & 0x4e \\
\hline
LATIN CAPITAL LETTER N	& N & 0x4e	& 0x4e & 	0x4e	& 0x4e \\
\hline 
MATHEMATICAL ITALIC CAPITAL N & $\mathcal{N}$	& 0xd835  0xdc41 &	0xd835  0xdc41 &	0x4e	 & 0x4e \\
\hline 
ROMAN NUMERAL TWELVE	& {\unicodefont Ⅻ}	&0x216b &	0x216b	& 0x58 0x49 0x49 &	0x58 0x49 0x49 \\
\hline
LATIN CAPITAL LETTER X - LATIN CAPITAL LETTER I - LATIN CAPITAL LETTER I &	XII &	0x58 0x49 0x49 &	0x58 0x49 0x49&	0x58(X) 0x49(I) 0x49(I) &	0x58 0x49 0x49 \\
\hline
VULGAR FRACTION ONE QUARTER	& \char"00BC &	0xbc &	0xbc &	0x31(1) 0x2044(/) 0x34(4) &	0x31 0x2044 0x34 \\
\hline
DIGIT ONE - SOLIDUS - DIGIT 4 &	1⁄4 &	0x31  0x2f 0x34 &	0x31 0x2f 0x34	& 0x31 0x2f 0x34 &	0x31 0x2f 0x34\\
\hline
DIGIT ONE - SOLIDUS - DIGIT 4 &	1⁄4 &	0x31  0x2f 0x34 &	0x31 0x2f 0x34	& 0x31 0x2f 0x34 &	0x31 0x2f 0x34\\
\hline
DIGIT ONE - FRACTION SLASH - DIGIT 4 & 1⁄4 &	0x31 0x2044 0x34 &	0x31 0x2044 0x34 &	0x31 0x2044 0x34 &	0x31 0x2044 0x34 \\
\hline
THERE EXISTS	& {\unicodefont ∃}&	0x2203 &	0x2203 &	0x2203 &	0x2203 \\
\hline
THERE DOES NOT EXIST	& {\unicodefont ∄} & 	0x2204	& 0x2203 0x338& 	0x2204 &	0x2203 0x338\\
\hline
LATIN CAPITAL LETTER OPEN E	& {\unicodefont Ɛ} &	0x190 &	0x190 &	0x190 &	0x190 \\
\hline
EULER CONSTANT	& {\unicodefont ℇ} & 0x2107 & 0x2107 & 0x190 & 0x190 \\
\hline
PLANCK CONSTANT	& {\unicodefont ℎ} &	0x210e &	0x210e &	0x68 &	0x68 \\
\hline
LATIN SMALL LETTER H &	h &	0x68 &	0x68 &	0x68 &	0x68 \\
\hline
PLANCK CONSTANT OVER TWO PI	& {\unicodefont ℏ} &	0x210f &	0x210f	& 0x127 &	0x127 \\
\hline
LATIN SMALL LETTER H WITH STROKE & ħ &	0x127	& 0x127	& 0x127	& 0x127 \\
\hline
GREEK CAPITAL LETTER OMEGA	& Ω &	0x3a9 &	0x3a9 &	0x3a9 &	0x3a9 \\
\hline
OHM SIGN & Ω &	0x2126 &	0x2126 &	0x3a9 &	0x3a9 \\
\hline
INTEGRAL & {\unicodefont ∫} &	0x222b &	0x222b &	0x222b &	0x222b \\
\hline
DOUBLE INTEGRAL	& {\unicodefont ∬} & 	0x222c &	0x222c &	0x222b 0x222b &	0x222b 0x222b \\
\hline
CONTOUR INTEGRAL	& {\unicodefont ∮} &	0x222e	 &0x222e &	0x222e &	0x222e \\
\hline
NABLA	& {\unicodefont ∇} &	0x2207	 &0x2207 &	0x2207 &	0x2207 \\
\hline
WHITE DOWN-POINTING TRIANGLE	& {\unicodefont ▽} &	0x25bd	 &0x25bd &	0x25bd &	0x25bd \\
\hline

\caption{Characters with similar glyphs and/or semantics}
\label{unicode_characters}
\end{longtable}


Since our goal is to match as much as possible similar characters, our system employs the NFKD representation of a character. For a given token, we test whether if it is already equal to its NFKD. If not, for all the characters that are not combining nor control characters, we add the token into the same position as the original one.
For example, given the token <mi>\r{A}</mi>, this will lead to the sequence <mi>\r{A}</mi><mi>A</mi> and the token <mi>{\unicodefontⅫ}</mi> will produce <mi>{\unicodefontⅫ}</mi><mi>X</mi><mi>I</mi><mi>I</mi>.

\subsubsection{Synonym Expansion}
Even though the Unicode normalization step work for a big part of the cases presented in table \ref{unicode_characters}, there are still some groups of characters which would not be matched (Even partially), such as some types of integrals. For addressing these groups, a precomputed table was created for some general groups of characters which have some similar meaning. For each token that belongs to some of this predefined groups, a second token is created identifying the group.
For example the token <mo>{\unicodefont ∮}</mo> is expanded into <mo>{\unicodefont ∮}</mo><mo>INTEGRALS</mo>, similarly the token token <mo>{\unicodefont ∫}</mo> is expanded into <mo>{\unicodefont ∫}</mo><mo>INTEGRALS</mo> and then both will have a partial match in the INTEGRALS token. 
Unicode specification does not define a strict rule to group similar characters based on their semantics. For general operators, we used the grouping provided by Xah Lee in his website\cite{math_groups}.

\subsubsection{Numeric Approximation} 
Some equations relate specific numeric quantities. While most of the previously explorer systems, handled <mn> tags by only representing them as a constant identifier, this approach is not suitable. A lot of articles in CDS relate to specific experiments under certain conditions, and require a high level of precision in the measurements. However, different experiments of the same phenomenon, can result in different measurements and we wanted to include this variability in our system. To handle this, when a token with a certain numeric quantity is discovered, we apply iterative rounding and index all the different approximations in the same position as the original element. For example the token <mn>2.0135</mn> will produce the following sequence of tokens: <mn>2.0135</mn><mn>2.014</mn><mn>2.01</mn><mn>2.0</mn>. 


\subsection{Structural Features}


\subsubsection{Algebraic structures}
The first set of structures consist on purely algebraic relationships between the expressions.
Inspired by predefined integral tables for different kinds of expressions, we decided to extract from a given equation, whether it has the occurrence of these kind of patterns.
An example of this type of structures would be: $X\textunderscore*(X\textunderscore + A\textunderscore)$ where $X\textunderscore$ and $A\textunderscore$ can be any type of expression. Sample equations matching this pattern can be $C (C+1)$ or $\frac{(\sin{\theta}\cos{\theta})^2}{2a} (\frac{(\sin{\theta}\cos{\theta})^2}{2a} + \tan{\frac{\theta+\phi}{2}})$. This structural analysis of equations is more powerful than the unification of variables and constants proposed in previously discussed projects because these patterns, as shown in the second example, can match an arbitrary complex expression. The drawback of our solution is the need to manually select representative and useful patterns. This selection was done based on standard based tables of integrals similar to the ones that Computational Algebraic Systems employ to compute them.

\subsubsection{Non-formal mathematical variations}
This set of structures represent common operations that cat appear in a given expression, but its identification is based on notation factors. A simple example of this can be the matrix decomposition $A = VDV^{-1}$ where the actual name of the variables $A$ and $V$ is not meaningful but the real importance is the fact that a given variable appears alongside its inverse in a matrix multiplication form. We would like to assign a partial match to any other kind of this decomposition say $B = MRM^{-1}$. these types of structures live in-between the notational and structural categories but, as it will be discussed, we identify them using Mathematica and encompass them as structural features.

\subsubsection{Specific domain structures}
Additionally, we identified patterns that are relevant and frequent in specific domains of the high energy physics.


\subsection{Development with Lucene/Solr}
This section explains how the described features were implemented in the {\codefont Apache Solr/Lucene} framework.
\subsection{Integration with Mathematica}
This section describes some aspects that should be had to be considered while implementing the integration with the Mathematica CAS. Our system uses the provided {\codefont JLink} library to create a connection with the underlying process {\codefont MathKernel}. The main class for communicating java code and {\codefont Mathematica} is {\codefont KernelLink} and the main used method was {\codefont evaluateToOutputForm(String expr, int pageWidth)} where {\codefont expr} is the expression to be evaluated and {\codefont pageWidth} affects the formatting of the result.

\subsubsection{Pattern Extraction}

\subsubsection{Recursive interpretation}

\subsubsection{Error handling}
Sometimes, after certain time, the connection between {\codefont KernelLink} and {\codefont Mathematica} will be lost launch an exception and following invocations would fails as well.
Also, re-instantiating a KernelLink will yield an Exception. For this scenario, killing the associated processes to Mathematica was necessary with {\codefont killall -s 9 Mathematica; killall -s 9 MathKernel
} or {\codefont taskkill /im Mathematica.exe; taskkill /im MathKernel.exe} depending on the OS the system is running on.

With this, the next creation of a new KernelLink works properly and the subsequent invocations work as expected.

\subsubsection{Timeout Handling}
Unfortunately, checking the quality of a LaTeX document is a difficult job and users have a lot of freedom in the way they write their document since the only visible result is the rendered version. 
One example we found during our work is the following TeX snippet:
\begin{verbatim}
	"\ \varphi\ are\ shown,\ as\ well\ as\ the\ weight\ and\ tension"
\end{verbatim}
A human can easily identify that this corresponds to text and that should be inserted into a proper "mbox" or "text" tag. In this case, the snippet produced the following MathML code:
\small{\codefont <math><mrow><mi>φ</mi><mo></mo><mrow><mi>a</mi><mo></mo>
<mi>r</mi><mo></mo><mi>e</mi></mrow><mo></mo><mrow><mi>s
</mi><mo></mo><mi>h</mi><mo></mo><mi>o</mi><mo></mo><mi>
w</mi><mo></mo><mi>n</mi></mrow><mo>,</mo><mrow><mi>a</m
i><mo></mo><mi>s</mi></mrow><mo></mo><mrow><mi>w</mi><mo
></mo><mi>e</mi><mo></mo><mi>l</mi><mo></mo><mi>l</mi></
mrow><mo></mo><mrow><mi>a</mi><mo></mo><mi>s</mi></mrow>
<mo></mo><mrow><mi>t</mi><mo></mo><mi>h</mi><mo></mo><mi
>e</mi></mrow><mo></mo><mrow><mi>w</mi><mo></mo><mi>e</m
i><mo></mo><mi>i</mi><mo></mo><mi>g</mi><mo></mo><mi>h</
mi><mo></mo><mi>t</mi></mrow><mo></mo><mrow><mi>a</mi><m
o></mo><mi>n</mi><mo></mo><mi>d</mi></mrow><mo></mo><mro
w><mi>t</mi><mo></mo><mi>e</mi><mo></mo><mi>n</mi><mo></
mo><mi>s</mi><mo></mo><mi>i</mi><mo></mo><mi>o</mi><mo><
/mo><mi>n</mi></mrow></mrow></math>
}

Mathematica took around 10 minutes to interpret it.
In the ideal case we would be able to identify the malformed expressions and pass to Mathematica only well-formed equations to be processed but as this simple example shows, that is a very challenging task and may require more work.
As a remedial situation, we used a fixed timeout to stop the processing of a given expression.

\subsection{Integration with CDS}
